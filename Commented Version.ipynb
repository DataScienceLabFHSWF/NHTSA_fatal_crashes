{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f7fb04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from urllib.request import urlopen\n",
    "from zipfile import ZipFile\n",
    "import pandas as pd\n",
    "import os\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "import string\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24b4d5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {\n",
    "    \"url\": 'https://www.nhtsa.gov/file-downloads?p=nhtsa/downloads/FARS/',   #Download URL\n",
    "    \"start_year\": 2000,                    #first year  for which data should be downloaded\n",
    "    \"national\": False,                     #include the national data\n",
    "    \"puerto_rico\": False,                  #include the data from Puerto Rico\n",
    "    \"auxiliary\": False,                    #include the auxiliary data\n",
    "    \"data_path\": './data/',                #path where the data is stored \n",
    "    \"write_all\": False,                    #determines if all tables are to be writen\n",
    "    \"selected_tables\": ['vehicle.csv'],    #list of tabels to write, needs to have .csv ending\n",
    "    \"delete_existing\":True,                #determines if writing a table should overwrite existing table with same name\n",
    "    \"start_year_writing\": 2000             #the year which si the fiirst, from which data is written\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d3113c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "site = requests.get(configs['url']).text\n",
    "soup = BeautifulSoup(site,'html.parser')\n",
    "hyperlinks = soup.find(\"tbody\").findChildren(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ceeff491",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperlink_list = []\n",
    "#this for loop finds all years, for which data is available\n",
    "for i in hyperlinks:\n",
    "    b = i['href'][38:42]\n",
    "    if b.isnumeric() and int(b)>= configs['start_year']:\n",
    "        hyperlink_list.append(b)\n",
    "        \n",
    "hyperlink_list = list(dict.fromkeys(hyperlink_list)) # this is for avoiding duplicate years    \n",
    "for i in range(0,len(hyperlink_list)):\n",
    "    hyperlink_list[i] = configs['url'] + hyperlink_list[i]+\"/\"\n",
    "# after this for loop there the hyperlink_list contains links to the download area for every year since 1975 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61c614e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#replaces has sturcture (table,columns,years,to_replace,replace)\n",
    "# table specifies the table to alter\n",
    "# columns specifies the columns to alter as an iterable, if it is just one column, iterable of length one\n",
    "# years is an iterable containing all the years where the alteration is to be applied\n",
    "# to_replace is the value to be replaced\n",
    "# replace is the replacing value\n",
    "replaces=[\n",
    "    ('vehicle',['trav_sp'],[2000,2001,2002,2003,2004,2005,2006,2007,2008],98,998),\n",
    "    ('vehicle',['trav_sp'],[2000,2001,2002,2003,2004,2005,2006,2007,2008],99,999)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d37e824c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#modifications has structure (table,columns,years,alteration,upper_bound)\n",
    "# table specifies the table to alter\n",
    "# columns specifies the columns to alter\n",
    "# years is an iterable containing all the years where the alteration is to be applied\n",
    "# alteration specifies the alterations which shall take place\n",
    "    # must be cointaining x in the mathematical expression i.e. inch to cm  \"x * 2.56\"\n",
    "    # must be a string --> within \"\" or ''\n",
    "# upper_bound specifies the upper bound with numbers larger than the upper bound not altered, to avoid changing i.e. codes for unknown values\n",
    "\n",
    "modifications = [\n",
    "    ('vehicle',['trav_sp'],[0],'round(x*1.609,0)',250)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "23f17b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#deletes has structure(table,columns,years,lower_bound,upper_bound,replace)\n",
    "# table specifies the table to alter\n",
    "# columns specifies the columns to alter\n",
    "# years is an iterable containing all the years where the alteration is to be applied\n",
    "# lower_bound specifies the lower bound of the range of values which is replaced, the bound is also replaced\n",
    "# upper_bound specifies the upper bound of the range of values which is replaced, the bound is also replaced\n",
    "# replace specifies the replacing value, None if no numerical value is wanted\n",
    "deletes = [\n",
    "    ('vehicle',['dr_hgt'],[0],90,107,None)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63f58d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(link_list,path):\n",
    "    #function expects a list containing the hyperlinks in string format\n",
    "    #path is the path where the data from the corresponding link list should be saved, should be string format\n",
    "    year_list = list(range(configs['start_year'],len(link_list)+configs['start_year'],1))\n",
    "    for i in range(0,len(link_list)):\n",
    "        with urlopen(link_list[i]) as zipresp:\n",
    "            with ZipFile(BytesIO(zipresp.read())) as zfile:\n",
    "                zfile.extractall(path+str(year_list[i]).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "104b01ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this downloads all the files according to the configs dictionary\n",
    "#links to the zip-files are constructed based on the hyperlink_list\n",
    "#the links to the zip-files are discarded after downloading all files of one group\n",
    "temp_list = []\n",
    "if configs['national']:\n",
    "    for i in hyperlink_list:\n",
    "        temp_list.append((i+'National/'+'FARS'+i[-5:-1]+'NationalCSV.zip').replace('www','static').replace('file-downloads?p=',''))\n",
    "    download(temp_list,\"data/standard/national/\")\n",
    "\n",
    "temp_list = []\n",
    "if configs['puerto_rico']:\n",
    "    for i in hyperlink_list:\n",
    "        temp_list.append((i+'Puerto%20Rico/'+'FARS'+i[-5:-1]+'PuertoRicoCSV.zip').replace('www','static').replace('file-downloads?p=',''))\n",
    "    download(temp_list,\"data/standard/puerto_rico/\")\n",
    "\n",
    "temp_list = []\n",
    "if configs['auxiliary']:\n",
    "    for i in hyperlink_list:\n",
    "        temp_list.append((i+'National/'+'FARS'+i[-5:-1]+'NationalAuxiliaryCSV.zip').replace('www','static').replace('file-downloads?p=',''))\n",
    "    download(temp_list,\"data/auxiliary/national/\")\n",
    "\n",
    "temp_list = []\n",
    "if configs['auxiliary'] and configs['puerto_rico']:\n",
    "    for i in hyperlink_list:\n",
    "        temp_list.append((i+'Puerto%20Rico/'+'FARS'+i[-5:-1]+'PuertoRicoAuxiliaryCSV.zip').replace('www','static').replace('file-downloads?p=',''))\n",
    "    download(temp_list,\"data/auxiliary/puerto_rico/\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d0bd444",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function adds the data to the database, frist trying to append the data to an existing table\n",
    "#if the table does not exist a new table will be created\n",
    "def add_to_database(dataframe,table,con_engine):\n",
    "    try:\n",
    "        #this will fail if there is a new column\n",
    "        dataframe.to_sql(name=table, con=con_engine, if_exists = 'append', index=False)\n",
    "    except:\n",
    "        #first all data from the table is querried out of the database, then the concatenated data is writen \n",
    "        #into the database, overwriting any existing table with the name\n",
    "        data = pd.read_sql('SELECT * FROM '+table, con_engine)\n",
    "        df2 = pd.concat([data,dataframe])\n",
    "        df2.to_sql(name=table, con=con_engine, if_exists = 'replace', index=False)#,method = 'psql_insert_copy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29378909",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filepaths():\n",
    "    #generates a list of paths to all files inside the data directory\n",
    "    filepaths = []\n",
    "    for root,dirs,files in os.walk(configs['data_path']):\n",
    "        for i in files:\n",
    "            filepaths.append(os.path.join(root,i))\n",
    "    return filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a92af024",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def timestamp(row):\n",
    "    #generates a unix timestamp, works for the accidents table\n",
    "    if row['day'] <31:\n",
    "        day = row['day']\n",
    "    else:\n",
    "        day = 1\n",
    "    if row['hour'] <24:\n",
    "        hour = row['hour']\n",
    "    else:\n",
    "        hour = 0\n",
    "    if row['minute'] <60:\n",
    "        minute = row ['minute']\n",
    "    else: \n",
    "        minute = 0    \n",
    "    return pd.Timestamp(row['year'],row['month'],day,hour,minute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b947e59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36ebecf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def primarykey(pk_df,information):\n",
    "    #this function creates primarykeys which are truly unique across all datarows, as the unique identifiers\n",
    "    #in the underlying data are reused every year\n",
    "    \n",
    "    if information[1] == 'accident':\n",
    "        pk_df['TIMESTAMP']= pk_df.apply(timestamp,axis = 1)\n",
    "        #this adds a new column with a timestamp to the accidents table/dataframe\n",
    "        \n",
    "    if 'st_case' in pk_df.columns:\n",
    "        front_part = information[0]*1000000\n",
    "        pk_df['primary_key_case'] = pk_df['st_case']+front_part\n",
    "        pk_df = change_col_position(['primary_key_case'],0,pk_df)\n",
    "        #ST_CASE is the unique identifier of the accidents table, it is a 5 to 6 digits integer number\n",
    "        #it is added to a 10 digit integer number with 6 trailing zeroes and the front being the year \n",
    "        #in which the data was gathered\n",
    "\n",
    "    if 'veh_no' in pk_df.columns and 'per_no' in pk_df.columns:\n",
    "        pk_df = pk_df.assign(uni_id = lambda x : (x['primary_key_case']*1000000 + x['veh_no']*1000 + x['per_no'])) \n",
    "    elif 'veh_no' in pk_df.columns:\n",
    "        pk_df = pk_df.assign(uni_id = lambda x : (x['primary_key_case']*1000000 + x['veh_no']*1000)) \n",
    "    elif 'per_no' in pk_df.columns:\n",
    "        pk_df = pk_df.assign(uni_id = lambda x : (x['primary_key_case']*1000000 + x['veh_no']*1000 + x['per_no'])) \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "    return pk_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e379025a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_col_position(colnames,position,dataframe):\n",
    "    #this function changes the position of a dataframe column\n",
    "    #expects an iterable for colnames, an integer for the position in the dataframe and a dataframe to be changed\n",
    "    collist = list(dataframe.columns)\n",
    "    for i in colnames: collist.remove(i)\n",
    "    for i in reversed(colnames): collist.insert(position,i)\n",
    "    return dataframe[collist]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0221a019",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_table(information,engine):\n",
    "    try:\n",
    "        sql = 'DROP TABLE IF EXISTS ' + information[1]\n",
    "        engine.execute(sql)\n",
    "        print('table '+information[1] + ' deleted')\n",
    "    except:\n",
    "        print('table '+information[1] + ' doesnt exist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081d8e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = get_filepaths() #paths is a list of paths for every file in every directory in the data directory\n",
    "name_set = set()        #name_set is a set containing every unique filename   \n",
    "engine = create_engine(\"postgresql+psycopg2://postgres:admin@localhost/NHTSA_FARS_NATIONAL\") \n",
    "# engine for database connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c655b308",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'paths' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[43mpaths\u001b[49m:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m#adds all files to a set, also takes the different capitalizations of the files into account\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     name_set\u001b[38;5;241m.\u001b[39madd(i\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mlower())\n\u001b[0;32m      6\u001b[0m name_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(name_set)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'paths' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for i in paths:\n",
    "    #adds all files to a set, also takes the different capitalizations of the files into account\n",
    "    name_set.add(i.split('\\\\')[-1].lower())\n",
    "  \n",
    "    \n",
    "name_list = list(name_set)\n",
    "name_list.sort()\n",
    "#print(name_list)\n",
    "\n",
    "if not configs['write_all']:\n",
    "    name_list = configs['selected_tables']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in name_list:\n",
    "    \n",
    "\n",
    "    \n",
    "    same_table_path_list = [x for x in paths if '\\\\'+i.lower() in x.lower()]\n",
    "    #print(same_table_path_list)\n",
    "    #list comprehension to find the paths to all csv files for the elements in the name_list\n",
    "    # the double backslash is important, to avoid confusion between certain tables having similar endings\n",
    "    frame_list = []\n",
    "    for j in same_table_path_list:\n",
    "        information = []\n",
    "        split_str = j.split('/')[-1].split(\"\\\\\")\n",
    "        information.append(int(split_str[-2]))\n",
    "        information.append(split_str[-1].split(\".\")[0].lower())\n",
    "        print(information)\n",
    "        #information is a list of 2 elements, first is the year of the dataframe/file, second is the name\n",
    "        \n",
    "        df_each_year = pd.read_csv(j,low_memory = False,encoding='latin_1')\n",
    "        #low_memory = True leads to a warning, to avoid that, it is set to false, workaround is pending\n",
    "        \n",
    "        df_each_year.columns = df_each_year.columns.str.lower()\n",
    "        #column names are not consistent in capitlization across the different years, fixed here\n",
    "        \n",
    "        #goes through the replace list to find all replace instructions for the specific data of the dataframe\n",
    "        for k in replaces:\n",
    "            if information[1] == k[0] and (information[0] in k[2] or k[2][0]==0) and k[1]:\n",
    "                for l in k[1]:\n",
    "                    df_each_year[l]= df_each_year[l].replace(k[3],k[4])\n",
    "            \n",
    "        #\n",
    "        for k in deletes:\n",
    "            if information[1] == k[0] and (information[0] in k[2] or k[2][0]==0):\n",
    "                for l in k[1]:\n",
    "                    df_each_year[l] = df_each_year[l].apply(lambda x: x if x<k[3] or x>k[4] else k[5])\n",
    "    \n",
    "    \n",
    "    \n",
    "        #goes through the modifications list and applys those to the corresponding tables in the corresponding years\n",
    "        for k in modifications:\n",
    "            if information[1] == k[0] and (information[0] in k[2] or k[2][0]==0):\n",
    "                #lambda_func = eval('lambda x: '+k[3]+'if x <'+ str(k[4])+'else None')\n",
    "                for l in k[1]:\n",
    "                    df_each_year[l] = df_each_year[l].apply(func = (lambda x:eval(k[3]+'if x <'+ str(k[4])+'else x')))\n",
    "                    \n",
    "        \n",
    "\n",
    "        \n",
    "        df_each_year = primarykey(df_each_year,information)\n",
    "        #the primarykey function is applied to every file individually\n",
    "        #this allows to replace the st_case key, which is not individual across years with primary_key_case\n",
    "        \n",
    "        \n",
    "        \n",
    "        if information[0] >=configs['start_year_writing']:\n",
    "            frame_list.append(df_each_year)\n",
    "\n",
    "    df = pd.concat(frame_list) #create one big dataframe out of the different small ones\n",
    "    \n",
    "    table_name = i.split('.')[0]# discards the csv ending\n",
    "    \n",
    "    if configs['delete_existing']:\n",
    "        delete_table(information,engine)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    max_df_elements = 80000000    #number will need adaptation for different machines, determines the size\n",
    "                                  #after which subsets of the dataframe are used for database insertion\n",
    "    \n",
    "    if df.shape[0]*df.shape[1]>max_df_elements :                   \n",
    "        print(df.shape[0])                                 \n",
    "        \n",
    "        chunk_size = int(max_df_elements / -530) \n",
    "        #chunk size is tested with a 32GB RAM Windows System, adaptations might be needed\n",
    "        #needs to be negative to generate the correct number in the range of the for-loop\n",
    "\n",
    "        \n",
    "        #this start at the back of the table/with the most recent data, to have values in as many columns as possible\n",
    "        #due to a drastic reduction in exection time compared to chronological inserts in the pandas to_sql function\n",
    "        for end in range(df.shape[0],0, chunk_size):\n",
    "            \n",
    "            start = end + chunk_size\n",
    "            if start < 0: #check for the start to avoid error in the iloc function\n",
    "                start = 0\n",
    "                \n",
    "            df_subset = df.iloc[start:end]\n",
    "            add_to_database(df_subset,information[1],engine)\n",
    "\n",
    "            del df_subset\n",
    "\n",
    "    else:\n",
    "        #this is for small and medium sized tabels\n",
    "        add_to_database(df,information[1],engine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2cf7c0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d0f348",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7300cf68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0790016a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0203264b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21beb0df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e172ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69dfdca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "if 2 in range (1,10,1):\n",
    "    print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cdc57b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1eea680",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686ffd3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac0524b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c614ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347c1d5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78504a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36076598",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
